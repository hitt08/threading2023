{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cb3251b-d046-49a0-855e-decc7accb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1164262e-05c3-4def-b40c-4c20f63296a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 20:33:22.911335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 20:33:23.493839: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-28 20:33:23.493913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-28 20:33:23.493920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "###### Enable if debugging, to reflect changes in .py files without restarting kernel #####\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "from data.docxparser import getCCD, getAnnotations\n",
    "from data.ccdParser import parseDocument, parseParagraphs\n",
    "from data.data_processing import clean_text\n",
    "from data.dateparser import datetime_parsing\n",
    "from utils.data_utils import write_json_dump,write_dict,write,load_config\n",
    "from data.vectorise_docs import vectorize_docs\n",
    "\n",
    "from extract_5w1h.extract_5w1h import run_5w1h_extract\n",
    "from extract_5w1h.vectorise_5w1h import vectorise_5w1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3df37cf-e0c2-4083-b69c-e7c1dec64fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "93dba4ec-9646-4af6-9a90-4e060df1a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_override={}\n",
    "\n",
    "config=load_config(overrides=config_override)\n",
    "\n",
    "##Additional Parameter\n",
    "# overrides = <dict> \n",
    "# -example: To set source/target directories\n",
    "# config_override = dict(\n",
    "#                        source_dir = \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/92Folder\",\n",
    "#                        target_dir = \"/nfs/jup/sensitivity_classifier/threading/e2e_data/\"\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cb98dade-c8cb-4181-b626-f8a5e02e7951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(source_dir='/app/92Folder',\n",
       "          target_dir='/app/DocumentSets',\n",
       "          nltk_dir='/app/nltk_data',\n",
       "          transformer_dir='/app/transformer_models',\n",
       "          emb_dir='/app/DocumentSets/embeddings',\n",
       "          data5w1h_dir='/app/DocumentSets/5w1h',\n",
       "          threads_dir='/app/DocumentSets/threads',\n",
       "          seqint_dir='/app/DocumentSets/seqint',\n",
       "          hint_dir='/app/DocumentSets/hint',\n",
       "          dpp_dir='/app/DocumentSets/dpp',\n",
       "          thread_sample_dir='/app/DocumentSets/sampling',\n",
       "          passage_dict_file='/app/DocumentSets/passages.jsonl.gz',\n",
       "          passage_docids_file='/app/DocumentSets/passage_doc_ids.txt.gz',\n",
       "          passage_parts_file='/app/DocumentSets/passages_parts.txt',\n",
       "          date_dict_file='/app/DocumentSets/date_date.txt.gz',\n",
       "          data5w1h_dict_file='/app/DocumentSets/5w1h/data_5w1h.json.gz',\n",
       "          entity_dict_file='/app/DocumentSets/5w1h/who_where_entities.json.gz',\n",
       "          date_features_file='/app/DocumentSets/5w1h/dt_feat.npz',\n",
       "          data5w1h_docids_file='/app/DocumentSets/5w1h/data_5w1h_docids.txt.gz',\n",
       "          data5w1h_parts_file='/app/DocumentSets/5w1h/data_5w1h_parts.txt',\n",
       "          passage_tfidf_features_file='/app/DocumentSets/embeddings/psg_tfidf_features.txt.gz',\n",
       "          passage_tfidf_emb_file='/app/DocumentSets/embeddings/psg_tfidf_emb.npz')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6fe1a-333a-46a6-b65f-a855aa4cd975",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parse Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fa8a996c-8665-4ae6-84da-094300cb13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self,config=None,default_date=None,auto_extract_dates=False) -> None:\n",
    "        if config is None:\n",
    "            config=load_config()\n",
    "        \n",
    "        self.config=config\n",
    "        self.log = logging.getLogger(__name__)\n",
    "        self.default_date = pd.to_datetime('2000-01-01 00:00:00') if default_date is None else default_date    #Can be used in case there is no date field    \n",
    "        self.auto_extract_dates = auto_extract_dates\n",
    "        \n",
    "\n",
    "    #READ DOCS\n",
    "    def read_docs(self,pathCCD,pathJSON):\n",
    "        config=self.config\n",
    "\n",
    "        files_to_process = []\n",
    "\n",
    "        # getting a list of all #wc.docx files in subfolders of target folder\n",
    "        for subdir, dirs, files in os.walk(config.source_dir):\n",
    "            for file in files:\n",
    "                filepath = subdir + os.sep + file\n",
    "                if filepath.endswith(\"#wc.docx\") and not filepath.endswith(\"#wcmeta.docx\"):  # ignore meta files\n",
    "                    files_to_process = files_to_process + [filepath]\n",
    "\n",
    "        # going through list and trying to create a ccd and json from each discovered file\n",
    "        failed_files = []\n",
    "        successful_files = []\n",
    "\n",
    "        self.log.warning(f\"Reading {len(files_to_process)} files\")\n",
    "        # get filenames and use docx parser to get ccd and json\n",
    "        for filepath in tqdm(files_to_process):\n",
    "            filename = os.path.basename(filepath)\n",
    "\n",
    "            ccd = getCCD(filepath)\n",
    "            jsn = getAnnotations(filepath)\n",
    "            json_string = json.dumps(jsn)\n",
    "\n",
    "            # TO DO: make ccdParser return 2 for json or ccd if either of those parsing processes fail for a doc\n",
    "\n",
    "            # if parsing file processes successfully, save ccd and json\n",
    "            if 2 not in [ccd, json]:\n",
    "                with open(os.path.join(pathCCD, os.path.splitext(filename)[0] + \".xml\"), 'w') as f:\n",
    "                    f.write(ccd)\n",
    "                    # f.close\n",
    "                with open(os.path.join(pathJSON, os.path.splitext(filename)[0] + \".json\"), 'w') as f:\n",
    "                    f.write(json_string)\n",
    "                successful_files = successful_files + [filepath]\n",
    "\n",
    "            else:\n",
    "                # i.e. if parsing failed add to failed files\n",
    "                failed_files = failed_files + [filepath]\n",
    "\n",
    "        return successful_files, failed_files\n",
    "\n",
    "    def get_passages(self,successful_files,min_len=10,max_len=500):\n",
    "        config=self.config\n",
    "        paraTitles = []\n",
    "        paraCollection = []\n",
    "        paraLabels = []\n",
    "        paraCreated = []\n",
    "\n",
    "        totPassages=0\n",
    "\n",
    "        self.log.warning(f\"Parsing passages from {len(successful_files)} files\")\n",
    "        # for filepaths to files that were succesfully parsed from ccd\n",
    "        for filepath in tqdm(successful_files):\n",
    "            # get filename from path\n",
    "            fileN = filepath.rsplit('/', 1)[-1][0:-5]\n",
    "\n",
    "            ####### CCD Document Parser #########\n",
    "            # call paragraph parser which opens ccd and json on its own\n",
    "            temp_paragarphs, temp_labelsP, doc_created = parseParagraphs(config.target_dir, fileN)\n",
    "            ####################################\n",
    "            \n",
    "            # Assigning a dummy date or extract date from text in case the documents do not have create-datetime field\n",
    "            if doc_created is None:\n",
    "                candidates = []\n",
    "                if self.auto_extract_dates:\n",
    "                    for p in temp_paragarphs:\n",
    "                        candidates,_ = datetime_parsing(p,self.default_date)\n",
    "                        if len(candidates): #Break after the first paragraph with dates\n",
    "                            break\n",
    "                doc_created=candidates[0] if len(candidates) else self.default_date #Select the first date object\n",
    "            \n",
    "            # Assigning dummy labels in case the documents do not have sensitivity ground-truth\n",
    "            if temp_labelsP is None:\n",
    "                temp_labelsP=np.ones(len(temp_paragarphs),dtype=int).tolist()\n",
    "\n",
    "            totPassages+=len(temp_paragarphs)\n",
    "\n",
    "            #Filter Passages based on number of words\n",
    "            paragarphs, labelsP,titles=[],[],[]\n",
    "            para_id=0\n",
    "            for p,l in zip(temp_paragarphs, temp_labelsP):\n",
    "                words=p.split()\n",
    "                if len(words) >= min_len and len(words) <= max_len:\n",
    "                    paragarphs.append(clean_text(p))\n",
    "                    labelsP.append(l)\n",
    "                    titles.append(fileN+\"_\"+str(para_id))\n",
    "                para_id+=1\n",
    "\n",
    "            # each paragraph that is returned needs to be linked back to its filename, so ([fileName] * (no_of_paras_returned))\n",
    "            # tempPara = [fileN+\"_\"+str(i) for i in range(len(paragarphs))]\n",
    "\n",
    "            tempCreated = [doc_created] * len(paragarphs)\n",
    "\n",
    "            # add the titles, paragraphs and labels for each para to respective holding lists\n",
    "            paraTitles = paraTitles + titles\n",
    "            paraCollection = paraCollection + paragarphs\n",
    "            paraLabels = paraLabels + labelsP\n",
    "            paraCreated = paraCreated + tempCreated\n",
    "\n",
    "        self.log.warning(f\"Total Passages: {totPassages}. Filtered Passages (length between [{min_len},{max_len}]): {len(paraCollection)}\")\n",
    "\n",
    "\n",
    "        # once all docs have been parsed to paragraphs and stored in the above three lists, convert to dataframe\n",
    "        # dataframes need to have [sourcefile, text, label]\n",
    "        paragraphDF = pd.DataFrame({'doc_id': paraTitles, 'text': paraCollection, 'label': paraLabels, 'created': paraCreated}).sort_values([\"created\", \"doc_id\"])\n",
    "\n",
    "        return paragraphDF\n",
    "\n",
    "    def process(self,min_len=10, max_len=500,collection_split_size=40000,mockup=False):\n",
    "        config=self.config\n",
    "        \n",
    "        #######CCD#########\n",
    "        pathCCD = os.path.join(config.target_dir,\"ccd\")\n",
    "        pathJSON = os.path.join(config.target_dir,\"json\")\n",
    "        \n",
    "        if not os.path.exists(pathCCD):\n",
    "            os.mkdir(pathCCD)\n",
    "        if not os.path.exists(pathJSON):\n",
    "            os.mkdir(pathJSON)\n",
    "\n",
    "        successful_files, failed_files = self.read_docs(pathCCD, pathJSON)\n",
    "        #######CCD#########\n",
    "        \n",
    "        \n",
    "        \n",
    "        paragraphDF = self.get_passages(successful_files, min_len=min_len, max_len=max_len)\n",
    "\n",
    "        paragraphDF[\"created\"] = paragraphDF[\"created\"].apply(lambda x: str(x) if x is not None else x)\n",
    "        write(config.passage_docids_file,paragraphDF[\"doc_id\"].values,mode=\"wt\",compress=True)\n",
    "        write_json_dump(config.passage_dict_file, paragraphDF.to_dict(orient=\"records\"))\n",
    "        write_dict(config.date_dict_file,paragraphDF.set_index(\"doc_id\")[\"created\"].to_dict(),mode=\"wt\",compress=True)\n",
    "        paragraphDF[\"created\"] = pd.to_datetime(paragraphDF[\"created\"])\n",
    "\n",
    "\n",
    "        if mockup:\n",
    "            #START MOCKUP\n",
    "            self.log.warning(\"\\n::::MOCKING DATA::::\\n\")\n",
    "            mock_passages=pickle.load(open(os.path.join(config.source_dir,\"aug.p\"),\"rb\"))\n",
    "            paragraphDF = paragraphDF[:3]\n",
    "            st_ind=np.max(paragraphDF.index.values)+1\n",
    "            doc_id = 0\n",
    "            df_data = []\n",
    "            for i in range(4):\n",
    "                st=i*300\n",
    "                for p1, p2, p3 in zip(mock_passages[st:st+100], mock_passages[st+100:st+200], mock_passages[st+200:st+300]):\n",
    "                    y, m, d = np.random.randint(1995, 2000), np.random.randint(1, 12), np.random.randint(1, 28)\n",
    "                    df_data.append([f\"doc{doc_id}#wc_{0}\", p1, np.random.choice([0,1],1)[0], \"{}-{:02}-{:02}\".format(y, m, d)])\n",
    "                    df_data.append([f\"doc{doc_id}#wc_{1}\", p2, np.random.choice([0,1],1)[0], \"{}-{:02}-{:02}\".format(y, m, d)])\n",
    "                    df_data.append([f\"doc{doc_id}#wc_{2}\", p3, np.random.choice([0,1],1)[0], \"{}-{:02}-{:02}\".format(y, m, d)])\n",
    "                    doc_id+=1\n",
    "\n",
    "            temp=pd.DataFrame(df_data, columns=paragraphDF.columns, index=range(st_ind, st_ind + len(df_data)))\n",
    "            temp[\"created\"] = pd.to_datetime(temp[\"created\"])\n",
    "            paragraphDF = pd.concat([paragraphDF, temp])\n",
    "\n",
    "            paragraphDF[\"created\"] = paragraphDF[\"created\"].apply(lambda x: str(x) if x is not None else x)\n",
    "            write(config.passage_docids_file,paragraphDF[\"doc_id\"].values,mode=\"wt\",compress=True)\n",
    "            write_json_dump(config.passage_dict_file, paragraphDF.to_dict(orient=\"records\"))\n",
    "            write_dict(config.date_dict_file,paragraphDF.set_index(\"doc_id\")[\"created\"],mode=\"wt\",compress=True)\n",
    "            paragraphDF[\"created\"] = pd.to_datetime(paragraphDF[\"created\"])\n",
    "            #### END MOCKUP ###\n",
    "\n",
    "\n",
    "        #Identifying Collection Splits, i.e., Parts\n",
    "        st = 0\n",
    "        en = collection_split_size\n",
    "        pidx = 0\n",
    "        parts = {}\n",
    "        while st < paragraphDF.shape[0]:\n",
    "            parts[pidx] = (st, st + paragraphDF.iloc[st:en].shape[0])\n",
    "            pidx += 1\n",
    "            st = en\n",
    "            en = st + collection_split_size\n",
    "\n",
    "        write_dict(config.passage_parts_file, parts)\n",
    "        self.log.warning(f\"Identified {len(parts)} parts (i.e., splits) of the collection.\")\n",
    "\n",
    "\n",
    "        return paragraphDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775186e-604e-4f6a-9441-df79b8dd6a6e",
   "metadata": {},
   "source": [
    "### Read Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0779eb11-e40a-4e3a-b1a1-bac489498d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor(config)\n",
    "\n",
    "\n",
    "##Additional Parameters\n",
    "#default_date = pd.to_datetime('2000-01-01')  #To be used as document create date in case there is no create_date attribute\n",
    "#auto_extract_dates = True                    #To extract date from document text in case there is no create_date attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ad119268-52f9-4111-a221-9d4a2161d827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 10 files\n",
      "100%|██████████| 10/10 [00:00<00:00, 27.30it/s]\n",
      "Parsing passages from 10 files\n",
      "100%|██████████| 10/10 [00:00<00:00, 466.22it/s]\n",
      "Total Passages: 70. Filtered Passages (length between [10,500]): 30\n",
      "Identified 1 parts (i.e., splits) of the collection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Passage: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doca#wc_3</td>\n",
       "      <td>1. This is to confirm (belatedly) that a new ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doca#wc_4</td>\n",
       "      <td>2. You should also be aware that the distribu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doca#wc_5</td>\n",
       "      <td>3. I would be grateful if you could pass a co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>docb#wc_3</td>\n",
       "      <td>1. This is to confirm (belatedly) that a new ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>docb#wc_4</td>\n",
       "      <td>2. You should also be aware that the distribu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-02-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id                                               text  label  \\\n",
       "0  doca#wc_3   1. This is to confirm (belatedly) that a new ...      1   \n",
       "1  doca#wc_4   2. You should also be aware that the distribu...      1   \n",
       "2  doca#wc_5   3. I would be grateful if you could pass a co...      1   \n",
       "3  docb#wc_3   1. This is to confirm (belatedly) that a new ...      1   \n",
       "4  docb#wc_4   2. You should also be aware that the distribu...      1   \n",
       "\n",
       "     created  \n",
       "0 1995-02-06  \n",
       "1 1995-02-06  \n",
       "2 1995-02-06  \n",
       "3 1995-02-06  \n",
       "4 1995-02-06  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_split_size=40000        #If passages are more than this number then the threading process will be splitted into batches\n",
    "paragraphDF = data_processor.process(collection_split_size=collection_split_size)\n",
    "\n",
    "print(f\"Total Passage: {paragraphDF.shape[0]}\")\n",
    "paragraphDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4128ac5-0fe8-4eab-a88f-c706606c4b7e",
   "metadata": {},
   "source": [
    "### Vectorise Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccecc5c2-3676-4a69-9ccf-80669ba12daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Vectorise\n",
      "\tTFIDF Vectors Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/psg_tfidf_emb.npz\n",
      "\tTFIDF-LSA Vectors Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/psg_tfidflsa_emb.npz\n",
      "\tTFIDF Features Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/psg_tfidf_features.txt.gz\n",
      "MiniLM Vectorise\n",
      "\tRunning on GPU\n",
      "100%|██████████| 1203/1203 [00:01<00:00, 644.72it/s]\n",
      "\tMiniLM Vectors Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/psg_minilm_emb.npz\n"
     ]
    }
   ],
   "source": [
    "vectorize_docs(config, vect_tfidf=True,vect_minilm=True,vect_roberta=False,sbert_batch=10000, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6391c1a-7b7c-435a-9846-e2b655b2158c",
   "metadata": {},
   "source": [
    "## 5W1H Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc04144-2361-4668-9d1a-fd17060b227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4931fc40-4f98-44a3-809e-d7f308352349",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using GPU for Constituency parsing.\n",
      "Running spaCy 4-way parallel (Total CPUs: 32)\n",
      "  0%|          | 0/1203 [00:00<?, ?it/s]Could not find corpus for WordNet, will now try to download the corpus.\n",
      "[nltk_data] Downloading package wordnet to /app/nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Batch: 1/2:   0%|          | 0/1203 [00:04<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/miniconda/lib/python3.8/site-packages/torch/distributions/distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "Batch 1/2 . Errors: 0:  13%|█▎        | 157/1203 [00:14<00:47, 22.22it/s]Traceback (most recent call last):\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extract_5w1h.py\", line 131, in annotate_extract_spacy\n",
      "    doc = extractor.parse(doc)\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extractor.py\", line 114, in parse\n",
      "    raise e\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extractor.py\", line 22, in run\n",
      "    extractor.process(document)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/abs_extractor.py\", line 40, in process\n",
      "    self._extract_candidates(document)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/cause_extractor.py\", line 92, in _extract_candidates\n",
      "    for candidate in self._evaluate_tree(tree):\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/cause_extractor.py\", line 240, in _evaluate_tree\n",
      "    if i != j and candidate[2] == candidates[j][2] and substring in candidate_strings[i]:\n",
      "IndexError: list index out of range\n",
      "Batch 1/2 . Errors: 1:  52%|█████▏    | 629/1203 [00:34<00:24, 23.47it/s]Traceback (most recent call last):\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extract_5w1h.py\", line 131, in annotate_extract_spacy\n",
      "    doc = extractor.parse(doc)\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extractor.py\", line 114, in parse\n",
      "    raise e\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extractor.py\", line 22, in run\n",
      "    extractor.process(document)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/abs_extractor.py\", line 40, in process\n",
      "    self._extract_candidates(document)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/cause_extractor.py\", line 92, in _extract_candidates\n",
      "    for candidate in self._evaluate_tree(tree):\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/cause_extractor.py\", line 240, in _evaluate_tree\n",
      "    if i != j and candidate[2] == candidates[j][2] and substring in candidate_strings[i]:\n",
      "IndexError: list index out of range\n",
      "Batch 1/2 . Errors: 2:  82%|████████▏ | 983/1203 [00:49<00:08, 24.77it/s]Traceback (most recent call last):\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extract_5w1h.py\", line 131, in annotate_extract_spacy\n",
      "    doc = extractor.parse(doc)\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extractor.py\", line 114, in parse\n",
      "    raise e\n",
      "  File \"/nfs/jup/sensitivity_classifier/threading/e2e_2023/sourceCode/extract_5w1h/extractor.py\", line 22, in run\n",
      "    extractor.process(document)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/abs_extractor.py\", line 40, in process\n",
      "    self._extract_candidates(document)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/cause_extractor.py\", line 92, in _extract_candidates\n",
      "    for candidate in self._evaluate_tree(tree):\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/Giveme5W1H/extractor/extractors/cause_extractor.py\", line 240, in _evaluate_tree\n",
      "    if i != j and candidate[2] == candidates[j][2] and substring in candidate_strings[i]:\n",
      "IndexError: list index out of range\n",
      "Batch 2/2 . Errors: 0: 100%|██████████| 1203/1203 [01:01<00:00, 19.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract 5w1h\n",
    "#The cell can resume from the last completed batch\n",
    "\n",
    "nlp_model=\"en_core_web_sm\" #Spacy Model, or use \"coreNLP\"\n",
    "n_processes=4              #Number of CPU Processes (The count will be adjusted to maximum available CPUs if n_processes>cpu_count())\n",
    "use_gpu=True               #Only for spaCy pipeline, ignored if torch.cuda.is_available() returns False\n",
    "\n",
    "run_5w1h_extract(config,n_processes=n_processes,nlp_model=nlp_model,use_gpu=use_gpu,skip_where=True,force=True)\n",
    "\n",
    "\n",
    "##Additional Parameters\n",
    "# => force=True        ##To delete all cached results and perform extraction from all files\n",
    "# => skip_where=True    ##To skip evaluation of location (where) from geopy\n",
    "# => skip_errors=False ##To report any error and kill the process\n",
    "# => show_errors=True  ##To report full errors\n",
    "# => threaded_extraction=False ##To extract 5w1h sequentially for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502330ee-aab6-4bc3-9b24-77e062823bdf",
   "metadata": {},
   "source": [
    "### Vectotise 5W1H Pseudo Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47be13d6-3d5d-4fc8-bc6f-ba7969e84372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating data dictionaries\n",
      "100%|██████████| 4/4 [00:00<00:00, 55738.26it/s]\n",
      "TFIDF Vectorise\n",
      "100%|██████████| 1200/1200 [00:01<00:00, 812.83it/s]\n",
      "\tTFIDF Vectors Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/tfidf_5w1h_emb.npz\n",
      "\tTFIDF-LSA Vectors Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/tfidflsa_5w1h_emb.npz\n",
      "\tTFIDF Features Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/tfidf_5w1h_features.txt.gz\n",
      "\tTokenised Collection Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/5w1h/tk_5w1h.json.gz\n",
      "MiniLM Vectorise\n",
      "\tRunning on GPU\n",
      "100%|██████████| 1200/1200 [00:01<00:00, 630.20it/s]\n",
      "\tMiniLM Vectors Saved at: /nfs/jup/sensitivity_classifier/threading/e2e_data/embeddings/minilm_5w1h_emb.npz\n"
     ]
    }
   ],
   "source": [
    "vectorise_5w1h(config, lsa_dim=200, vect_tfidf=True,vect_minilm=True,vect_roberta=False,sbert_batch=10000,use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bb4df-06c0-464a-ac75-e9f2161eee58",
   "metadata": {},
   "source": [
    "## Date Extraction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "65250dab-cd0e-4440-9416-902f4a388ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"5-Jan-2012 17:00 \\n2011 Haiti Earthquake Anniversary. As of 2010 (see 1500 photos here), the following major earthquakes \"\\\n",
    "    \"have been recorded in Haiti. The first great earthquake mentioned in histories of Haiti occurred in \"\\\n",
    "    \"1564 in what was still the Spanish colony. It destroyed Concepción de la Vega. On January 12, 2010, \"\\\n",
    "    \"a massive earthquake struck the nation of Haiti, causing catastrophic damage inside and around the \"\\\n",
    "    \"capital city of Port-au-Prince. On the first anniversary of the earthquake, 12 January 2011, \"\\\n",
    "    \"Haitian Prime Minister Jean-Max Bellerive said the death toll from the quake in 2010 was more \"\\\n",
    "    \"than 316,000, raising the figures in 2010 from previous estimates. I immediately flashed back to the afternoon \"\\\n",
    "    \"of 11th Feb, 1975 when, on my car radio, I first heard the news. On Sunday morning of the following week...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "64b51545-63df-4bb2-9526-bdd017b50074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference date: 2023-02-28 20:57:42.948026\n",
      "\tDatetime: \"2012-01-05 17:00:00\" extracted from text span: \"5-Jan-2012 17:00\"\n",
      "\tDatetime: \"2011-01-12 00:00:00\" extracted from text span: \"12 January 2011\"\n",
      "\tDatetime: \"1975-02-11 00:00:00\" extracted from text span: \"11th Feb, 1975\"\n",
      "\tDatetime: \"2010-01-12 00:00:00\" extracted from text span: \"January 12, 2010\"\n",
      "\tDatetime: \"2023-03-07 00:00:00\" extracted from text span: \"following week\"\n",
      "\tDatetime: \"2023-03-05 00:00:00\" extracted from text span: \"Sunday\"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "base_date = datetime.now()\n",
    "print(f\"Reference date: {base_date}\")\n",
    "datetime_objects,text_spans = datetime_parsing(text,base_date)\n",
    "for d,s in zip(datetime_objects,text_spans):\n",
    "    print(f\"\\tDatetime: \\\"{d}\\\" extracted from text span: \\\"{s}\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
